---
layout: post
title: "Self-Instruct for Wekeza LLM (v2): Bootstrapping Kenya-Specific Financial AI"
date: 2025-07-26
categories: [machine-learning, kenya]
tags: [llm, self-instruct, fine-tuning, wekeza]
---
# Self-Instruct for Wekeza LLM (v2): Bootstrapping Kenya-Specific Financial AI

## üìö Understanding Self-Instruct

I tried implementing the **Self-Instruct** methodology from the research paper ["Self-Instruct: Aligning Language Models with Self-Generated Instructions"](https://arxiv.org/abs/2212.10560) to enhance my Wekeza LLM project. This framework demonstrates how to improve instruction-following capabilities in language models without requiring any human-annotated instruction data.

The core innovation is straightforward: start with a small seed of instructions and let the model generate its own training data through bootstrapped fine-tuning. It's simple yet incredibly effective for domain-specific applications.

## üéØ Why Self-Instruct Was Perfect for Wekeza LLM

Wekeza LLM is my specialized lightweight model designed to answer **Kenya-specific investment questions**, with a particular focus on **money market funds**. The challenge I faced was clear: standard instruction datasets don't contain the nuanced, localized financial information that Kenyan investors need.

Self-Instruct offered the perfect solution:

- **Generate realistic, diverse financial Q&A pairs** tailored to the Kenyan market
- **Scale organically** from my initial v1 dataset to a more robust v2
- **Simple implementation** that worked seamlessly with my existing infrastructure

## üöÄ The Journey: From 92 to 142 Examples

**My Goal**: Transform Wekeza LLM's ability to handle complex, localized investment prompts through strategic instruction fine-tuning.

**The Result**: I successfully expanded my training dataset from **92 ‚Üí 142 examples** using Self-Instruct principles, then fine-tuned the model to create v2. The improvement was immediately noticeable‚Äîthe new model handles nuanced financial questions with significantly better accuracy and contextual understanding.

## üõ†Ô∏è Implementation Walkthrough

Let me walk you through exactly how I implemented this approach:

### Step 1: Loading the Foundation Dataset

```python
# Load original v1 dataset
input_path = "C:/Users/bbollo/Downloads/WekezaLLM_dataset_v1.jsonl"
```

### Step 2: Generating Synthetic Instructions

I took a manually-guided approach to ensure quality control over the generated content:

```python
from random import choice
import json

seed_instructions = [
    "How do I start investing in Kenya's unit trusts?",
    "List top money market funds with low risk.",
    "Explain management fees in MMFs in simple terms.",
    ...
]

# Model-generated synthetic responses (manually verified)
synthetic_data = []

for i, instruction in enumerate(seed_instructions):
    response = input(f"Write a helpful answer for: {instruction}\n> ")
    synthetic_data.append({
        "instruction": instruction,
        "input": "",
        "output": response
    })

# Save synthetic samples
with open("self_instruct_raw_v2.jsonl", "w") as f:
    for example in synthetic_data:
        f.write(json.dumps(example) + "\n")
```

### Step 3: Merging Datasets

```python
# Merge original and synthetic
with open("WekezaLLM_dataset_v1.jsonl") as f1, open("self_instruct_raw_v2.jsonl") as f2:
    original = [json.loads(l) for l in f1]
    synthetic = [json.loads(l) for l in f2]

combined = original + synthetic

# Save as v2 dataset
with open("WekezaLLM_dataset_v2.jsonl", "w") as out:
    for ex in combined:
        out.write(json.dumps(ex) + "\n")

print(f"Combined dataset saved with {len(combined)} examples.")
```

### Step 4: Tokenizing the Enhanced Dataset

```python
from transformers import AutoTokenizer
from datasets import load_dataset

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
dataset = load_dataset("json", data_files="WekezaLLM_dataset_v2.jsonl")["train"]

# Preprocess
def tokenize(example):
    prompt = f"### Instruction:\n{example['instruction']}\n### Input:\n{example['input']}\n### Response:\n{example['output']}"
    return tokenizer(prompt, truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize)
```

### Step 5: Fine-tuning the v2 Model

```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling

model = AutoModelForCausalLM.from_pretrained("distilgpt2")

training_args = TrainingArguments(
    output_dir="./distilgpt2-wekeza-finetuned_v2",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    save_steps=50,
    save_total_limit=2,
    logging_steps=10
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

trainer.train()
```

### Step 6: Preserving the Improved Model

```python
save_path = "./distilgpt2-wekeza-finetuned_v2"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
```

## üìâ Training Loss Analysis

During the fine-tuning process, I monitored the training metrics closely. The final training output showed promising results:

```
TrainOutput(global_step=426, training_loss=2.3106516426158064, metrics={'train_runtime': 2309.2097, 'train_samples_per_second': 0.184, 'train_steps_per_second': 0.184, 'total_flos': 55656207876096.0, 'train_loss': 2.3106516426158064, 'epoch': 3.0})
```

The training completed in **38 minutes and 23 seconds** over **3 epochs** with **426 total steps**. Looking at the loss trajectory throughout training, I observed a clear downward trend:

- **Initial loss (step 10)**: ~3.409
- **Mid-training (step 200)**: ~2.239
- **Final loss (step 426)**: ~2.311

Key observations from the loss pattern:

- **Steady convergence**: The loss decreased from over 3.4 to around 2.3, indicating effective learning
- **No severe overfitting**: While there were fluctuations, the overall trend remained downward
- **Healthy learning rate**: The model adapted well to the Kenya-specific financial domain without getting stuck in local minima
- **Quality synthetic data**: The consistent improvement suggests the Self-Instruct generated data was relevant and well-structured

This loss reduction directly correlates with the improved performance I observed in the model's responses to complex financial queries. The final training loss of **2.31** represents a significant improvement from the initial values, demonstrating successful domain adaptation.

## üéØ See the Difference: v2 Model in Action

Here's a real example that showcases the improvement:

**Prompt:**
> What is the minimum amount I need to start investing in a money market fund in Kenya?

**Wekeza LLM v2 Response:**
> Most money market funds in Kenya have a minimum investment of KES 1,000 to KES 5,000, with some like CIC Money Market Fund starting at KES 1,000. Popular funds from Britam, Old Mutual, and ICEA allow you to start with as little as KES 1,000 and make additional contributions of KES 500 or more.

The response is specific, actionable, and demonstrates deep understanding of the Kenyan financial landscape.

## üîç Key Learnings and Reflections

This paper implementation was transformative for both my understanding and my project:

**Technical Growth**: I gained hands-on experience with instruction fine-tuning methodologies and learned how to bootstrap quality datasets from minimal starting points.

**Domain Expertise**: Successfully created Kenya-specific financial Q&A pairs that address real investor needs in the local market.

**Iterative Development**: Established a robust versioning system that allows me to compare models across iterations and track improvements.

**Foundation for Scaling**: Built the groundwork for future enhancements including evaluation frameworks and RAG integration.

## üìÇ Code and Resources

The complete implementation, datasets, and models are available in my GitHub repository:
üëâ **[Wekeza LLM Self-Instruct v2 Repository](https://github.com/Okoth67/wekeza-llm-selfinstruct-v2)**

The repository contains:
- Complete Jupyter notebook with step-by-step implementation
- Original v1 and enhanced v2 datasets
- Training scripts and model configurations
- Evaluation results and loss tracking

## üöÄ What's Next for Wekeza LLM

This papers opened up exciting learning possibilities:
- **Comprehensive Evaluation**: Re-Implementing IFScore or developing custom evaluation metrics for financial accuracy
- **Advanced Fine-tuning**: Exploring QLoRA and DPO techniques for even better performance


---

## ü§ñ AI-Enhanced Content Creation

This blog post was enhanced using multiple AI tools to ensure quality and clarity:
- **Grammarly** for grammar correction and writing refinement
- **ChatGPT** for content structure and organization
- **Claude** for debugging code snippets and technical accuracy
- **NotebookLM** to summarize and understand the original Self-Instruct paper

*Getting better....*
